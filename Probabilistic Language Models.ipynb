{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "790070c0",
   "metadata": {},
   "source": [
    "# ðŸ§ª PROG8245 - Probabilistic Language Models Workshop\n",
    "**Team Members:**\n",
    "- Parth Patel  \n",
    "- Fenil Patel (9001279)  \n",
    "- Adithya \n",
    "\n",
    "**Workshop Objective:**\n",
    "Implement NLP preprocessing and four probabilistic language models with structured code and documentation in a single Jupyter Notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97ddec1",
   "metadata": {},
   "source": [
    "\n",
    "#### Step 2: Imports and Setup (Code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6053cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK data paths:\n",
      " data\n",
      "data\n",
      "data\n",
      "data\n",
      "data\n",
      "C:\\Users\\parth/nltk_data\n",
      "c:\\Users\\parth\\OneDrive\\Desktop\\AIM2\\Lab8\\.venv\\nltk_data\n",
      "c:\\Users\\parth\\OneDrive\\Desktop\\AIM2\\Lab8\\.venv\\share\\nltk_data\n",
      "c:\\Users\\parth\\OneDrive\\Desktop\\AIM2\\Lab8\\.venv\\lib\\nltk_data\n",
      "C:\\Users\\parth\\AppData\\Roaming\\nltk_data\n",
      "C:\\nltk_data\n",
      "D:\\nltk_data\n",
      "E:\\nltk_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\parth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\parth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\parth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "\n",
    "# <<< EDIT THIS PATH IF NEEDED >>>\n",
    "NLTK_DIR = r\"data\"  \n",
    "\n",
    "# Make sure the folder exists\n",
    "os.makedirs(NLTK_DIR, exist_ok=True)\n",
    "\n",
    "# Download required packages *into this folder*\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Tell NLTK to look *first* in this folder\n",
    "nltk.data.path = [NLTK_DIR] + nltk.data.path\n",
    "\n",
    "print(\"NLTK data paths:\\n\", \"\\n\".join(nltk.data.path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8677d8ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "punkt path: C:\\Users\\parth\\AppData\\Roaming\\nltk_data\\tokenizers\\punkt\n",
      "stopwords path: C:\\Users\\parth\\AppData\\Roaming\\nltk_data\\corpora\\stopwords\n"
     ]
    }
   ],
   "source": [
    "# These should succeed silently if the data is there\n",
    "print(\"punkt path:\", nltk.data.find('tokenizers/punkt'))\n",
    "print(\"stopwords path:\", nltk.data.find('corpora/stopwords'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d20e5e",
   "metadata": {},
   "source": [
    "## Preprocessing: Tokenization and Normalization\n",
    "\n",
    "We apply lowercase conversion, tokenization, punctuation removal, and stopwordÂ filtering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2672b41",
   "metadata": {},
   "source": [
    "#### Tokenizer and Normalizer Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "196240d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)                # uses punkt\n",
    "    tokens = [t for t in tokens if t.isalpha()] # keep alphabetic only\n",
    "    sw = set(stopwords.words('english'))        # load stopwords once\n",
    "    tokens = [t for t in tokens if t not in sw]\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1fe426d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'doc1': ['natural',\n",
       "  'language',\n",
       "  'processing',\n",
       "  'allows',\n",
       "  'computers',\n",
       "  'understand',\n",
       "  'human',\n",
       "  'language'],\n",
       " 'doc2': ['language', 'models', 'predict', 'next', 'word', 'sequence'],\n",
       " 'doc3': ['probabilistic',\n",
       "  'models',\n",
       "  'estimate',\n",
       "  'likelihood',\n",
       "  'word',\n",
       "  'sequences'],\n",
       " 'doc4': ['workshop',\n",
       "  'covers',\n",
       "  'unigram',\n",
       "  'bigram',\n",
       "  'trigram',\n",
       "  'backoff',\n",
       "  'models']}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = {\n",
    "    'doc1': \"Natural language processing allows computers to understand human language.\",\n",
    "    'doc2': \"Language models predict the next word in a sequence.\",\n",
    "    'doc3': \"Probabilistic models estimate the likelihood of word sequences.\",\n",
    "    'doc4': \"This workshop covers unigram, bigram, trigram, and backoff models.\"\n",
    "}\n",
    "\n",
    "preprocessed_corpus = {doc_id: preprocess(text) for doc_id, text in corpus.items()}\n",
    "preprocessed_corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86af2c81",
   "metadata": {},
   "source": [
    "#### Inverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e07ad627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'human': ['doc1'],\n",
       " 'natural': ['doc1'],\n",
       " 'computers': ['doc1'],\n",
       " 'language': ['doc1', 'doc2'],\n",
       " 'understand': ['doc1'],\n",
       " 'processing': ['doc1'],\n",
       " 'allows': ['doc1'],\n",
       " 'predict': ['doc2'],\n",
       " 'sequence': ['doc2'],\n",
       " 'models': ['doc2', 'doc3', 'doc4'],\n",
       " 'word': ['doc2', 'doc3'],\n",
       " 'next': ['doc2'],\n",
       " 'probabilistic': ['doc3'],\n",
       " 'estimate': ['doc3'],\n",
       " 'likelihood': ['doc3'],\n",
       " 'sequences': ['doc3'],\n",
       " 'trigram': ['doc4'],\n",
       " 'unigram': ['doc4'],\n",
       " 'covers': ['doc4'],\n",
       " 'workshop': ['doc4'],\n",
       " 'backoff': ['doc4'],\n",
       " 'bigram': ['doc4']}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from collections import defaultdict\n",
    "inverted_index = defaultdict(set)\n",
    "\n",
    "for doc_id, tokens in preprocessed_corpus.items():\n",
    "    for token in set(tokens):  # Use set to avoid duplicate entries\n",
    "        inverted_index[token].add(doc_id)\n",
    "\n",
    "# Convert sets to sorted lists for readability\n",
    "inverted_index = {k: sorted(v) for k, v in inverted_index.items()}\n",
    "inverted_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287f6836",
   "metadata": {},
   "source": [
    "#### Unigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e4f86f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P('language') = 0.1111\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "class UnigramModel:\n",
    "    def __init__(self, corpus):\n",
    "        self.tokens = sum(corpus.values(), [])\n",
    "        self.freq = Counter(self.tokens)\n",
    "        self.total = len(self.tokens)\n",
    "\n",
    "    def prob(self, word):\n",
    "        return self.freq[word] / self.total if word in self.freq else 0\n",
    "\n",
    "# Usage\n",
    "unigram = UnigramModel(preprocessed_corpus)\n",
    "print(f\"P('language') = {unigram.prob('language'):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad426bf",
   "metadata": {},
   "source": [
    "#### Bigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "612cf642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P('language' | 'human') = 1.0000\n"
     ]
    }
   ],
   "source": [
    "class BigramModel:\n",
    "    def __init__(self, corpus):\n",
    "        self.bigrams = Counter()\n",
    "        self.unigrams = Counter()\n",
    "        for tokens in corpus.values():\n",
    "            self.unigrams.update(tokens)\n",
    "            self.bigrams.update(zip(tokens[:-1], tokens[1:]))\n",
    "\n",
    "    def prob(self, w1, w2):\n",
    "        return self.bigrams[(w1, w2)] / self.unigrams[w1] if self.unigrams[w1] else 0\n",
    "\n",
    "# Usage\n",
    "bigram = BigramModel(preprocessed_corpus)\n",
    "print(f\"P('language' | 'human') = {bigram.prob('human', 'language'):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c99dad4",
   "metadata": {},
   "source": [
    "#### Trigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bc5e8468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P('processing' | 'natural' | 'language') = 1.0000\n"
     ]
    }
   ],
   "source": [
    "class TrigramModel:\n",
    "    def __init__(self, corpus):\n",
    "        self.trigrams = Counter()\n",
    "        self.bigrams = Counter()\n",
    "        for tokens in corpus.values():\n",
    "            self.bigrams.update(zip(tokens[:-1], tokens[1:]))\n",
    "            self.trigrams.update(zip(tokens[:-2], tokens[1:-1], tokens[2:]))\n",
    "\n",
    "    def prob(self, w1, w2, w3):\n",
    "        return self.trigrams[(w1, w2, w3)] / self.bigrams[(w1, w2)] if self.bigrams[(w1, w2)] else 0\n",
    "\n",
    "# Usage\n",
    "trigram = TrigramModel(preprocessed_corpus)\n",
    "print(f\"P('processing' | 'natural' | 'language') = {trigram.prob('natural', 'language', 'processing'):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85eba242",
   "metadata": {},
   "source": [
    "#### Backoff Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164e5c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P('processing' | 'natural language') = 0.7037\n"
     ]
    }
   ],
   "source": [
    "class BackoffModel:\n",
    "    def __init__(self, corpus, lambda1=0.1, lambda2=0.3, lambda3=0.6):\n",
    "        self.uni = UnigramModel(corpus)\n",
    "        self.bi = BigramModel(corpus)\n",
    "        self.tri = TrigramModel(corpus)\n",
    "        self.lambda1 = lambda1\n",
    "        self.lambda2 = lambda2\n",
    "        self.lambda3 = lambda3\n",
    "\n",
    "    def prob(self, w1, w2, w3):\n",
    "        p3 = self.tri.prob(w1, w2, w3)\n",
    "        p2 = self.bi.prob(w2, w3)\n",
    "        p1 = self.uni.prob(w3)\n",
    "        return self.lambda3 * p3 + self.lambda2 * p2 + self.lambda1 * p1\n",
    "\n",
    "# Usage\n",
    "backoff = BackoffModel(preprocessed_corpus)\n",
    "print(f\"P('processing' | 'natural' | 'language') = {backoff.prob('natural', 'language', 'processing'):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aef1006",
   "metadata": {},
   "source": [
    "### Peer Talking Points â€“ Probabilistic Language Models\n",
    "\n",
    "Let's explore some key discussion questions based on each part of our workshop:\n",
    "\n",
    "---\n",
    "\n",
    "#### Preprocessing\n",
    "- Do you think using **lemmatization** instead of just lowercasing and removing stopwords would give us better results in a small dataset?\n",
    "- Should we always remove punctuation and numbers? Could they be useful in other NLP tasks like sentiment analysis or timestamps?\n",
    "\n",
    "---\n",
    "\n",
    "#### Inverted Index\n",
    "- In what ways does building an **inverted index** help with tasks like search and document retrieval?\n",
    "- By using `set(tokens)` to remove duplicates, are we potentially **losing frequency information** that might help in scoring or ranking documents?\n",
    "\n",
    "---\n",
    "\n",
    "#### Unigram Model\n",
    "- Since the **unigram model treats each word independently**, what are some limitations when trying to model natural language this way?\n",
    "- What happens if we ask the model to predict a word itâ€™s never seen before? Should we consider **smoothing techniques** like Laplace smoothing?\n",
    "\n",
    "---\n",
    "\n",
    "#### Bigram Model\n",
    "- How much does **word order** matter in bigram models? For example, is \"language model\" the same as \"model language\" to the model?\n",
    "- If we have a larger corpus, how can we deal with **rare bigrams** or completely missing word pairs?\n",
    "\n",
    "---\n",
    "\n",
    "#### Trigram Model\n",
    "- Does adding a third word (trigram) improve prediction significantly, or does it just make things **more sparse**?\n",
    "- Can we do anything to back off or cluster **unseen trigrams** into simpler bigram or unigram alternatives?\n",
    "\n",
    "---\n",
    "\n",
    "#### Backoff Model\n",
    "- How do the **lambda weights** in our backoff model influence the final prediction? Should they be fixed or learned?\n",
    "- Would using a **more advanced smoothing method** like Kneser-Ney or Katz improve accuracy in low-data settings?\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
